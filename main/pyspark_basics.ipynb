{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% [markdown]\n",
        "# # PySpark Basics with PostgreSQL Database\n",
        "# \n",
        "# This notebook demonstrates fundamental PySpark operations using data from our PostgreSQL database.\n",
        "# PySpark is Apache Spark's Python API, offering distributed computing capabilities for big data processing.\n",
        "# We'll cover:\n",
        "# 1. Spark session setup and database connection\n",
        "# 2. Basic DataFrame operations\n",
        "# 3. Data manipulation and cleaning\n",
        "# 4. Data sampling and analysis\n",
        "# 5. Data modeling basics\n",
        "# 6. Performance optimization techniques\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Setup and Spark Session Configuration\n",
        "# First, let's set up our Spark session and configure it for optimal performance.\n",
        "#\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, count, when, lit, round, desc, asc\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\n",
        "from pyspark.sql.window import Window\n",
        "import time\n",
        "from helpers.db_connection import DatabaseConnection, query_to_df\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create Spark session with optimized configurations\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PostgreSQL Analysis\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "    .config(\"spark.default.parallelism\", \"2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set display options\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumRows\", 10)\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", False)\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Exploring Database Tables\n",
        "# Let's first see what tables are available in our database.\n",
        "#\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Query to list all tables in the database\n",
        "tables_query = \"\"\"\n",
        "SELECT table_name \n",
        "FROM information_schema.tables \n",
        "WHERE table_schema = 'public'\n",
        "ORDER BY table_name;\n",
        "\"\"\"\n",
        "\n",
        "# Get list of tables\n",
        "tables_df = query_to_df(tables_query)\n",
        "print(\"Available tables in the database:\")\n",
        "print(tables_df)\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Exploring Table Structures\n",
        "# Let's examine the structure of each table.\n",
        "#\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to get column information for a table\n",
        "def get_table_structure(table_name):\n",
        "    query = f\"\"\"\n",
        "    SELECT column_name, data_type, character_maximum_length\n",
        "    FROM information_schema.columns\n",
        "    WHERE table_name = '{table_name}'\n",
        "    ORDER BY ordinal_position;\n",
        "    \"\"\"\n",
        "    return query_to_df(query)\n",
        "\n",
        "# Get structure for each table\n",
        "for table in tables_df['table_name']:\n",
        "    print(f\"\\nStructure of table '{table}':\")\n",
        "    print(get_table_structure(table))\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Loading Sample Data\n",
        "# Now that we know the table structure, let's load some sample data.\n",
        "#\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example query to get data from the first table\n",
        "sample_query = f\"\"\"\n",
        "SELECT *\n",
        "FROM {tables_df['table_name'].iloc[0]}\n",
        "LIMIT 1000;\n",
        "\"\"\"\n",
        "\n",
        "# Load data into a Spark DataFrame\n",
        "df = spark.createDataFrame(query_to_df(sample_query))\n",
        "\n",
        "# Display basic information about the DataFrame\n",
        "print(\"DataFrame Schema:\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\nDataFrame Summary Statistics:\")\n",
        "df.describe().show()\n",
        "\n",
        "print(\"\\nDataFrame Columns:\")\n",
        "print(df.columns)\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ### Viewing Data\n",
        "# Different ways to view the data in our Spark DataFrame.\n",
        "#\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display first 5 rows\n",
        "print(\"First 5 rows:\")\n",
        "df.show(5)\n",
        "\n",
        "# Display last 5 rows (using orderBy and limit)\n",
        "print(\"\\nLast 5 rows:\")\n",
        "df.orderBy(df.columns[0].desc()).limit(5).show()\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ### Basic Operations\n",
        "# Let's explore some basic Spark DataFrame operations.\n",
        "#\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select all columns\n",
        "print(\"All columns:\")\n",
        "df.select(\"*\").show(5)\n",
        "\n",
        "# Select specific columns\n",
        "numeric_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, (IntegerType, DoubleType))]\n",
        "if numeric_columns:\n",
        "    print(\"\\nNumeric columns:\")\n",
        "    df.select(numeric_columns).show(5)\n",
        "\n",
        "# Basic filtering\n",
        "if numeric_columns:\n",
        "    print(f\"\\nFiltered data (where {numeric_columns[0]} > 0):\")\n",
        "    df.filter(col(numeric_columns[0]) > 0).show(5)\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ### Aggregations and Grouping\n",
        "# Demonstrate aggregation operations if we have appropriate columns.\n",
        "#\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get categorical columns (string type)\n",
        "categorical_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
        "\n",
        "# If we have both numeric and categorical columns, show some aggregations\n",
        "if numeric_columns and categorical_columns:\n",
        "    print(f\"Aggregations grouped by {categorical_columns[0]}:\")\n",
        "    df.groupBy(categorical_columns[0]) \\\n",
        "      .agg(avg(numeric_columns[0]).alias(\"average\")) \\\n",
        "      .orderBy(\"average\", ascending=False) \\\n",
        "      .show()\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Data Export\n",
        "# How to export our processed data.\n",
        "#\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Export to CSV\n",
        "df.write.mode(\"overwrite\").csv(\"processed_data_spark\", header=True)\n",
        "\n",
        "# Export to Parquet (Spark's preferred format)\n",
        "df.write.mode(\"overwrite\").parquet(\"processed_data_spark\")\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Cleanup\n",
        "# Clean up resources and stop the Spark session.\n",
        "#\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stop Spark session\n",
        "spark.stop() "
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}