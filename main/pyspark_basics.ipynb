{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# # PySpark Basics with PostgreSQL Database\n",
        "# \n",
        "# This notebook demonstrates fundamental PySpark operations using data from our PostgreSQL database.\n",
        "# PySpark is Apache Spark's Python API, offering distributed computing capabilities for big data processing.\n",
        "# We'll cover:\n",
        "# 1. Spark session setup and database connection\n",
        "# 2. Basic DataFrame operations\n",
        "# 3. Data manipulation and cleaning\n",
        "# 4. Data sampling and analysis\n",
        "# 5. Data modeling basics\n",
        "# 6. Performance optimization techniques\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Setup and Spark Session Configuration\n",
        "# First, let's set up our Spark session and configure it for optimal performance.\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import col, avg, count, when, lit, round, desc, asc\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\n",
        "from pyspark.sql.window import Window\n",
        "import time\n",
        "from helpers.db_connection import DatabaseConnection, query_to_df\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create Spark session with optimized configurations\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PostgreSQL Analysis\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "    .config(\"spark.default.parallelism\", \"2\") \\\n",
        "    .getOrCreate()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available tables in the database:\n",
            "       table_name\n",
            "0       addresses\n",
            "1     assignments\n",
            "2  communications\n",
            "3         workers\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>table_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>addresses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>assignments</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>communications</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>workers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       table_name\n",
              "0       addresses\n",
              "1     assignments\n",
              "2  communications\n",
              "3         workers"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Query to list all tables in the database\n",
        "tables_query = \"\"\"\n",
        "SELECT table_name \n",
        "FROM information_schema.tables \n",
        "WHERE table_schema = 'public'\n",
        "ORDER BY table_name;\n",
        "\"\"\"\n",
        "\n",
        "# Get list of tables\n",
        "tables_df = query_to_df(tables_query)\n",
        "print(\"Available tables in the database:\")\n",
        "print(tables_df)\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Exploring Table Structures\n",
        "# Let's examine the structure of each table.\n",
        "#\n",
        "tables_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Structure of table 'addresses':\n",
            "         column_name          data_type  character_maximum_length\n",
            "0          unique_id               uuid                       NaN\n",
            "1   worker_unique_id               uuid                       NaN\n",
            "2       address_type  character varying                      50.0\n",
            "3      address_line1  character varying                     255.0\n",
            "4      address_line2  character varying                     255.0\n",
            "5               city  character varying                     100.0\n",
            "6              state  character varying                     100.0\n",
            "7        postal_code  character varying                      20.0\n",
            "8            country          character                       2.0\n",
            "9     effective_from               date                       NaN\n",
            "10      effective_to               date                       NaN\n",
            "\n",
            "Structure of table 'assignments':\n",
            "         column_name          data_type  character_maximum_length\n",
            "0          unique_id               uuid                       NaN\n",
            "1   worker_unique_id               uuid                       NaN\n",
            "2  assignment_number  character varying                      50.0\n",
            "3        position_id  character varying                      50.0\n",
            "4      department_id  character varying                      50.0\n",
            "5        location_id  character varying                      50.0\n",
            "6     effective_from               date                       NaN\n",
            "7       effective_to               date                       NaN\n",
            "\n",
            "Structure of table 'communications':\n",
            "        column_name          data_type  character_maximum_length\n",
            "0         unique_id               uuid                       NaN\n",
            "1  worker_unique_id               uuid                       NaN\n",
            "2      contact_type  character varying                      50.0\n",
            "3     contact_value  character varying                     255.0\n",
            "4      primary_flag          character                       1.0\n",
            "5    effective_from               date                       NaN\n",
            "6      effective_to               date                       NaN\n",
            "\n",
            "Structure of table 'workers':\n",
            "        column_name          data_type  character_maximum_length\n",
            "0         unique_id               uuid                       NaN\n",
            "1         person_id  character varying                      50.0\n",
            "2   employee_number  character varying                      50.0\n",
            "3        first_name  character varying                     100.0\n",
            "4         last_name  character varying                     100.0\n",
            "5        birth_date               date                       NaN\n",
            "6               sex          character                       1.0\n",
            "7    marital_status          character                       1.0\n",
            "8       nationality          character                       2.0\n",
            "9    effective_from               date                       NaN\n",
            "10     effective_to               date                       NaN\n"
          ]
        }
      ],
      "source": [
        "# Function to get column information for a table\n",
        "def get_table_structure(table_name):\n",
        "    query = f\"\"\"\n",
        "    SELECT column_name, data_type, character_maximum_length\n",
        "    FROM information_schema.columns\n",
        "    WHERE table_name = '{table_name}'\n",
        "    ORDER BY ordinal_position;\n",
        "    \"\"\"\n",
        "    return query_to_df(query)\n",
        "\n",
        "# Get structure for each table\n",
        "for table in tables_df['table_name']:\n",
        "    print(f\"\\nStructure of table '{table}':\")\n",
        "    print(get_table_structure(table))\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Loading Sample Data\n",
        "# Now that we know the table structure, let's load some sample data.\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tables_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example query to get data from the first table\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sample_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mSELECT *\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;124mFROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtables_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mLIMIT 1000;\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load data into a Spark DataFrame\u001b[39;00m\n\u001b[1;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(query_to_df(sample_query))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tables_df' is not defined"
          ]
        }
      ],
      "source": [
        "# Example query to get data from the first table\n",
        "sample_query = f\"\"\"\n",
        "SELECT *\n",
        "FROM {tables_df['table_name'].iloc[0]}\n",
        "LIMIT 1000;\n",
        "\"\"\"\n",
        "\n",
        "# Load data into a Spark DataFrame\n",
        "df = spark.createDataFrame(query_to_df(sample_query))\n",
        "\n",
        "# Display basic information about the DataFrame\n",
        "print(\"DataFrame Schema:\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\nDataFrame Summary Statistics:\")\n",
        "df.describe().show()\n",
        "\n",
        "print(\"\\nDataFrame Columns:\")\n",
        "print(df.columns)\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ### Viewing Data\n",
        "# Different ways to view the data in our Spark DataFrame.\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first 5 rows\n",
        "print(\"First 5 rows:\")\n",
        "df.show(5)\n",
        "\n",
        "# Display last 5 rows (using orderBy and limit)\n",
        "print(\"\\nLast 5 rows:\")\n",
        "df.orderBy(df.columns[0].desc()).limit(5).show()\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ### Basic Operations\n",
        "# Let's explore some basic Spark DataFrame operations.\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select all columns\n",
        "print(\"All columns:\")\n",
        "df.select(\"*\").show(5)\n",
        "\n",
        "# Select specific columns\n",
        "numeric_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, (IntegerType, DoubleType))]\n",
        "if numeric_columns:\n",
        "    print(\"\\nNumeric columns:\")\n",
        "    df.select(numeric_columns).show(5)\n",
        "\n",
        "# Basic filtering\n",
        "if numeric_columns:\n",
        "    print(f\"\\nFiltered data (where {numeric_columns[0]} > 0):\")\n",
        "    df.filter(col(numeric_columns[0]) > 0).show(5)\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ### Aggregations and Grouping\n",
        "# Demonstrate aggregation operations if we have appropriate columns.\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get categorical columns (string type)\n",
        "categorical_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
        "\n",
        "# If we have both numeric and categorical columns, show some aggregations\n",
        "if numeric_columns and categorical_columns:\n",
        "    print(f\"Aggregations grouped by {categorical_columns[0]}:\")\n",
        "    df.groupBy(categorical_columns[0]) \\\n",
        "      .agg(avg(numeric_columns[0]).alias(\"average\")) \\\n",
        "      .orderBy(\"average\", ascending=False) \\\n",
        "      .show()\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Data Export\n",
        "# How to export our processed data.\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to CSV\n",
        "df.write.mode(\"overwrite\").csv(\"processed_data_spark\", header=True)\n",
        "\n",
        "# Export to Parquet (Spark's preferred format)\n",
        "df.write.mode(\"overwrite\").parquet(\"processed_data_spark\")\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Cleanup\n",
        "# Clean up resources and stop the Spark session.\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Spark session\n",
        "spark.stop() "
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": ".new",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
