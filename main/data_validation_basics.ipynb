{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %% [markdown]\n",
        "# # Data Validation Basics with Pandera\n",
        "# \n",
        "# This notebook demonstrates data validation techniques using Pandera with different DataFrame libraries\n",
        "# (Pandas, Polars, and PySpark). We'll also compare Pandera with Great Expectations.\n",
        "# \n",
        "# ## What is Pandera?\n",
        "# \n",
        "# Pandera is a statistical data validation library for pandas DataFrames. It provides a flexible and\n",
        "# declarative way to validate data at runtime, ensuring data quality and consistency.\n",
        "# \n",
        "# ### Key Benefits of Pandera:\n",
        "# 1. **Lightweight and Fast**: Pandera is designed to be lightweight and performant, making it suitable\n",
        "#    for both development and production environments.\n",
        "# 2. **Type Hints and Schema Validation**: Provides Python type hints and runtime schema validation.\n",
        "# 3. **Integration with Multiple Libraries**: Works with Pandas, Polars, and PySpark.\n",
        "# 4. **Declarative API**: Easy to read and maintain validation rules.\n",
        "# 5. **Custom Validation Rules**: Supports custom validation functions and complex validation logic.\n",
        "# \n",
        "# ### Pandera vs Great Expectations\n",
        "# \n",
        "# While Pandera is excellent for many use cases, Great Expectations is often preferred in production\n",
        "# data pipelines for several reasons:\n",
        "# \n",
        "# **Great Expectations Advantages:**\n",
        "# 1. **Production-Ready**: Built specifically for data pipeline validation in production environments.\n",
        "# 2. **Data Documentation**: Generates comprehensive data documentation and profiling.\n",
        "# 3. **Data Quality Reports**: Creates detailed data quality reports and dashboards.\n",
        "# 4. **Integration with Data Platforms**: Better integration with data platforms and ETL tools.\n",
        "# 5. **Data Profiling**: Built-in data profiling capabilities.\n",
        "# 6. **Community and Support**: Larger community and more enterprise support.\n",
        "# \n",
        "# **When to Use Each:**\n",
        "# - **Pandera**: Use for lightweight validation in development, testing, or smaller projects.\n",
        "# - **Great Expectations**: Use for production data pipelines, large-scale projects, or when you need\n",
        "#   comprehensive data quality reporting.\n",
        "# \n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Setup and Imports\n",
        "# First, let's import the necessary libraries and set up our environment."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import polars as pl\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "import pandera as pa\n",
        "from pandera.typing import Series, DataFrame\n",
        "from typing import Optional\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from helpers.db_connection import DatabaseConnection, query_to_df\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Exploring Database Tables\n",
        "# Let's first see what tables are available in our database."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Query to list all tables in the database\n",
        "tables_query = \"\"\"\n",
        "SELECT table_name \n",
        "FROM information_schema.tables \n",
        "WHERE table_schema = 'public'\n",
        "ORDER BY table_name;\n",
        "\"\"\"\n",
        "\n",
        "# Get list of tables\n",
        "tables_df = query_to_df(tables_query)\n",
        "print(\"Available tables in the database:\")\n",
        "print(tables_df)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Exploring Table Structures\n",
        "# Let's examine the structure of each table."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to get column information for a table\n",
        "def get_table_structure(table_name):\n",
        "    query = f\"\"\"\n",
        "    SELECT column_name, data_type, character_maximum_length\n",
        "    FROM information_schema.columns\n",
        "    WHERE table_name = '{table_name}'\n",
        "    ORDER BY ordinal_position;\n",
        "    \"\"\"\n",
        "    return query_to_df(query)\n",
        "\n",
        "# Get structure for each table\n",
        "for table in tables_df['table_name']:\n",
        "    print(f\"\\nStructure of table '{table}':\")\n",
        "    print(get_table_structure(table))\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Define Data Schema Based on Available Tables\n",
        "# Let's create a schema that matches our actual database structure."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example query to get data from the first table\n",
        "sample_query = f\"\"\"\n",
        "SELECT *\n",
        "FROM {tables_df['table_name'].iloc[0]}\n",
        "LIMIT 5;\n",
        "\"\"\"\n",
        "\n",
        "# Load sample data to understand the structure\n",
        "sample_df = query_to_df(sample_query)\n",
        "print(\"Sample data structure:\")\n",
        "print(sample_df.info())\n",
        "\n",
        "# Define the schema based on actual columns\n",
        "class DataSchema(pa.SchemaModel):\n",
        "    \"\"\"Schema for data validation.\"\"\"\n",
        "    \n",
        "    # We'll dynamically add fields based on the actual columns\n",
        "    class Config:\n",
        "        \"\"\"Schema configuration.\"\"\"\n",
        "        strict = True\n",
        "        coerce = True\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Load and Validate Data with Pandas"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load data from the first available table\n",
        "query = f\"\"\"\n",
        "SELECT *\n",
        "FROM {tables_df['table_name'].iloc[0]}\n",
        "LIMIT 1000;\n",
        "\"\"\"\n",
        "\n",
        "# Load into Pandas DataFrame\n",
        "pandas_df = query_to_df(query)\n",
        "print(\"\\nLoaded data info:\")\n",
        "print(pandas_df.info())\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Load and Validate Data with Polars"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert to Polars DataFrame\n",
        "polars_df = pl.DataFrame(pandas_df)\n",
        "print(\"\\nPolars DataFrame info:\")\n",
        "print(polars_df.describe())\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Load and Validate Data with PySpark"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Data Validation\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "spark_df = spark.createDataFrame(pandas_df)\n",
        "print(\"\\nSpark DataFrame schema:\")\n",
        "spark_df.printSchema()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Data Quality Metrics"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def calculate_data_quality(df):\n",
        "    \"\"\"Calculate data quality metrics.\"\"\"\n",
        "    metrics = {\n",
        "        \"total_rows\": len(df),\n",
        "        \"missing_values\": df.isnull().sum().to_dict(),\n",
        "        \"unique_values\": df.nunique().to_dict(),\n",
        "        \"data_types\": df.dtypes.to_dict()\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Calculate metrics for Pandas DataFrame\n",
        "print(\"Data Quality Metrics:\")\n",
        "print(calculate_data_quality(pandas_df))\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Cleanup\n",
        "# Clean up resources and stop the Spark session."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stop Spark session\n",
        "spark.stop() "
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}